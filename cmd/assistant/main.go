package main

import (
	"context"
	"log"
	"os"
	"time"

	"github.com/joho/godotenv"
	"github.com/truong-nautilus/smart-home-ai/internal/infrastructure/edgetts"
	"github.com/truong-nautilus/smart-home-ai/internal/infrastructure/keyboard"
	"github.com/truong-nautilus/smart-home-ai/internal/infrastructure/media"
	"github.com/truong-nautilus/smart-home-ai/internal/infrastructure/ollama"
	"github.com/truong-nautilus/smart-home-ai/internal/infrastructure/phowhisper"
	"github.com/truong-nautilus/smart-home-ai/internal/infrastructure/wav2vec2"
	"github.com/truong-nautilus/smart-home-ai/internal/usecase"
	"github.com/truong-nautilus/smart-home-ai/pkg/logger"
)

func main() {
	// T·∫£i file .env n·∫øu c√≥
	if err := godotenv.Load(); err != nil {
		log.Println("‚ö†Ô∏è  Kh√¥ng t√¨m th·∫•y file .env, s·ª≠ d·ª•ng bi·∫øn m√¥i tr∆∞·ªùng h·ªá th·ªëng")
	}

	// Kh·ªüi t·∫°o c√°c ph·ª• thu·ªôc (local)
	consoleLogger := logger.NewConsoleLogger()
	ffmpeg := media.NewFFmpegCapturer()

	// Speech recognizer - ch·ªçn gi·ªØa PhoWhisper ho·∫∑c Wav2Vec2
	asrModel := os.Getenv("ASR_MODEL")
	if asrModel == "" {
		asrModel = "phowhisper" // m·∫∑c ƒë·ªãnh
	}

	var recognizer *phowhisper.PhoWhisperRecognizer
	var wav2vec2Recognizer *wav2vec2.Wav2Vec2Recognizer

	if asrModel == "wav2vec2" {
		// Wav2Vec2 recognizer (fast Vietnamese CTC model)
		wav2vec2Script := "/Users/phamthetruong/github/smart-home-ai/scripts/wav2vec2_transcribe.py"
		wav2vec2Recognizer = wav2vec2.NewWav2Vec2Recognizer(wav2vec2Script)
		consoleLogger.Info("üé§ S·ª≠ d·ª•ng Wav2Vec2 ASR")
	} else {
		// PhoWhisper recognizer (vinai/PhoWhisper - t·ªëi ∆∞u cho ti·∫øng Vi·ªát)
		phowhisperScript := os.Getenv("PHOWHISPER_SCRIPT")
		if phowhisperScript == "" {
			phowhisperScript = "/Users/phamthetruong/github/smart-home-ai/scripts/phowhisper_transcribe.py"
		}
		recognizer = phowhisper.NewPhoWhisperRecognizer(phowhisperScript)
		consoleLogger.Info("üé§ S·ª≠ d·ª•ng PhoWhisper ASR")
	}

	// Ollama local model
	ollamaModel := os.Getenv("OLLAMA_MODEL")
	if ollamaModel == "" {
		ollamaModel = "phi-3-mini"
	}
	aiClient := ollama.New(ollamaModel)

	// Edge TTS synthesizer (Microsoft neural TTS - gi·ªçng r·∫•t t·ª± nhi√™n)
	edgeTTSVoice := os.Getenv("EDGE_TTS_VOICE") // vi-VN-HoaiMyNeural (n·ªØ) ho·∫∑c vi-VN-NamMinhNeural (nam)
	edgeTTSBin := os.Getenv("EDGE_TTS_BIN")     // optional
	synthesizer := edgetts.New(edgeTTSVoice, edgeTTSBin)

	// Keyboard listener (Space key ƒë·ªÉ ghi √¢m)
	keyboardListener := keyboard.NewListener()

	// Use case (v·ªõi keyboard listener)
	var assistant *usecase.AssistantUseCase
	if wav2vec2Recognizer != nil {
		assistant = usecase.NewAssistantUseCase(
			ffmpeg,             // media capturer
			wav2vec2Recognizer, // speech recognizer (Wav2Vec2)
			aiClient,           // ai assistant (ollama)
			synthesizer,        // speech synthesizer (Edge TTS)
			keyboardListener,   // keyboard listener (Enter key)
			consoleLogger,
		)
	} else {
		assistant = usecase.NewAssistantUseCase(
			ffmpeg,           // media capturer
			recognizer,       // speech recognizer (PhoWhisper)
			aiClient,         // ai assistant (ollama)
			synthesizer,      // speech synthesizer (Edge TTS)
			keyboardListener, // keyboard listener (Enter key)
			consoleLogger,
		)
	}

	// Th·ª±c thi v√¥ h·∫°n - ch·∫ø ƒë·ªô press twice
	ctx := context.Background()
	consoleLogger.Info("üöÄ Tr·ª£ l√Ω AI ƒë√£ s·∫µn s√†ng!")
	consoleLogger.Info("üìå C√°ch d√πng: Nh·∫•n ENTER l·∫ßn 1 ‚Üí ghi √¢m ‚Üí nh·∫•n ENTER l·∫ßn 2 ‚Üí x·ª≠ l√Ω")
	consoleLogger.Info("üõë Nh·∫•n Ctrl+C ƒë·ªÉ tho√°t")

	for {
		if err := assistant.Execute(ctx); err != nil {
			consoleLogger.Error("‚ö†Ô∏è L·ªói khi th·ª±c thi", err)
			// Kh√¥ng tho√°t, ti·∫øp t·ª•c ch·∫°y
			consoleLogger.Info("üîÑ Kh·ªüi ƒë·ªông l·∫°i sau 2 gi√¢y...")
			time.Sleep(2 * time.Second)
			continue
		}

		// Ngh·ªâ 1 gi√¢y tr∆∞·ªõc khi ch·ªù gesture ti·∫øp theo
		consoleLogger.Info("‚ú® Ho√†n t·∫•t! S·∫µn s√†ng cho l·∫ßn ti·∫øp theo...")
		time.Sleep(1 * time.Second)
	}
}
