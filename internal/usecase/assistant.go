package usecase

import (
	"context"
	"fmt"
	"os"

	"github.com/truong-nautilus/smart-home-ai/internal/domain"
)

// GestureDetector phÃ¡t hiá»‡n cá»­ chá»‰ tá»« camera
type GestureDetector interface {
	WaitForTwoFingers(ctx context.Context) (bool, error)
}

// AssistantUseCase orchestrates the AI assistant workflow
type AssistantUseCase struct {
	gestureDetector   GestureDetector
	mediaCapturer     domain.MediaCapturer
	speechRecognizer  domain.SpeechRecognizer
	aiAssistant       domain.AIAssistant
	speechSynthesizer domain.SpeechSynthesizer
	logger            Logger
}

// Logger interface for logging
type Logger interface {
	Info(msg string)
	Error(msg string, err error)
}

// NewAssistantUseCase creates a new assistant use case
func NewAssistantUseCase(
	gestureDetector GestureDetector,
	mediaCapturer domain.MediaCapturer,
	speechRecognizer domain.SpeechRecognizer,
	aiAssistant domain.AIAssistant,
	speechSynthesizer domain.SpeechSynthesizer,
	logger Logger,
) *AssistantUseCase {
	return &AssistantUseCase{
		gestureDetector:   gestureDetector,
		mediaCapturer:     mediaCapturer,
		speechRecognizer:  speechRecognizer,
		aiAssistant:       aiAssistant,
		speechSynthesizer: speechSynthesizer,
		logger:            logger,
	}
}

// Execute runs the complete AI assistant workflow
func (uc *AssistantUseCase) Execute(ctx context.Context) error {
	const (
		imageFile     = "frame.jpg"
		audioFile     = "audio.wav"
		replyFile     = "reply.mp3"
		audioDuration = 5
	)

	// Cleanup temp files on exit
	defer uc.cleanup(imageFile, audioFile, replyFile)

	// Step 1: Wait for gesture trigger (chá» vÃ´ háº¡n)
	uc.logger.Info("ğŸ‘‹ HÃ£y giÆ¡ 2 ngÃ³n tay trÆ°á»›c camera Ä‘á»ƒ báº¯t Ä‘áº§u (Ä‘ang chá»...)...")
	detected, err := uc.gestureDetector.WaitForTwoFingers(ctx)
	if err != nil {
		return fmt.Errorf("khÃ´ng thá»ƒ phÃ¡t hiá»‡n cá»­ chá»‰: %w", err)
	}
	if !detected {
		return fmt.Errorf("khÃ´ng phÃ¡t hiá»‡n Ä‘Æ°á»£c cá»­ chá»‰ 2 ngÃ³n tay")
	}
	uc.logger.Info("âœ… ÄÃ£ phÃ¡t hiá»‡n cá»­ chá»‰ 2 ngÃ³n tay!")

	// Step 2: Capture image
	uc.logger.Info("ğŸ¥ Äang chá»¥p áº£nh tá»« camera...")
	if err := uc.mediaCapturer.CaptureImage(ctx, imageFile); err != nil {
		return fmt.Errorf("khÃ´ng thá»ƒ chá»¥p áº£nh: %w", err)
	}
	uc.logger.Info("âœ… Chá»¥p áº£nh thÃ nh cÃ´ng")

	// Step 3: Record audio
	uc.logger.Info("ğŸ¤ Äang ghi Ã¢m tá»« microphone (5 giÃ¢y)...")
	if err := uc.mediaCapturer.RecordAudio(ctx, audioFile, audioDuration); err != nil {
		return fmt.Errorf("khÃ´ng thá»ƒ ghi Ã¢m: %w", err)
	}
	uc.logger.Info("âœ… Ghi Ã¢m thÃ nh cÃ´ng")

	// Step 4: Transcribe audio
	uc.logger.Info("ğŸ§  Äang chuyá»ƒn giá»ng nÃ³i thÃ nh vÄƒn báº£n (whisper.cpp)...")
	transcription, err := uc.speechRecognizer.Transcribe(ctx, audioFile)
	if err != nil {
		return fmt.Errorf("khÃ´ng thá»ƒ chuyá»ƒn giá»ng nÃ³i: %w", err)
	}
	uc.logger.Info(fmt.Sprintf("ğŸ“ VÄƒn báº£n: \"%s\"", transcription.Text))

	// Step 5: Analyze with AI
	uc.logger.Info("ğŸ¤– Äang phÃ¢n tÃ­ch (ollama, mÃ´ hÃ¬nh local)...")
	response, err := uc.aiAssistant.AnalyzeMultimodal(ctx, transcription.Text, imageFile)
	if err != nil {
		return fmt.Errorf("khÃ´ng thá»ƒ nháº­n pháº£n há»“i tá»« AI: %w", err)
	}
	uc.logger.Info(fmt.Sprintf("ğŸ’¬ Pháº£n há»“i AI: \"%s\"", response.Text))

	// Step 6: Synthesize speech
	uc.logger.Info("ğŸ”Š Äang tá»•ng há»£p giá»ng nÃ³i (sá»­ dá»¥ng 'say' trÃªn macOS)...")
	if _, err := uc.speechSynthesizer.Synthesize(ctx, response.Text, replyFile); err != nil {
		return fmt.Errorf("khÃ´ng thá»ƒ tá»•ng há»£p giá»ng nÃ³i: %w", err)
	}
	uc.logger.Info("âœ… Tá»•ng há»£p giá»ng nÃ³i thÃ nh cÃ´ng")

	// Step 7: Play audio
	uc.logger.Info("ğŸ”ˆ Äang phÃ¡t Ã¢m thanh pháº£n há»“i...")
	if err := uc.mediaCapturer.PlayAudio(ctx, replyFile); err != nil {
		return fmt.Errorf("khÃ´ng thá»ƒ phÃ¡t Ã¢m thanh: %w", err)
	}

	uc.logger.Info("âœ… HoÃ n thÃ nh!")
	return nil
}

func (uc *AssistantUseCase) cleanup(files ...string) {
	for _, file := range files {
		if _, err := os.Stat(file); err == nil {
			os.Remove(file)
		}
	}
}
